# Agentic Postgres Workshop

This workshop demonstrates working with TigerData using Tiger MCP tools.

## Install Tiger CLI

Tiger-CLI is an open source cli tool and MCP server for managing and quiring database services. It also gives AI
Assistance tools like Claude Code access to Postgres and TigerData/TimescaleDB best practices.

Source code and documentation is avaliable at: <https://github.com/timescale/tiger-cli>

To install Tiger-CLI run in the terminal:

```bash
curl -fsSL https://cli.tigerdata.com | sh
```

Check the Tiger-CLI version:

```bash
tiger version
```

After installing Tiger CLI, authenticate with your Tiger Cloud account:

```bash
# Login to your Tiger account
tiger auth login
```

## Connect Tiger-CLI to your AI Assistnt tool

You can use any of the following code assist tools:

- Claude Code
- Codex
- Cursor
- Gemini CLI
- Google Antigravity
- Kiro-cli
- VS Code
- Windsurf

Most of these tools require payed subscription to the service suth as OpenAI or Anthropic. If you don't have any of the payed subscriptions you can use Gemini CLI from Google. It include generouse free tier, sufficient to complete this workshop.

To connect the tool to Tiger MCP use:

```bash
# Login to your Tiger account
tiger mcp install
```

If your tool is not listed, you still might be able to use it if it supports MCP via Stdio or HTTP. Refer to you tool's and TigerCLI documentation for MCP server integration.

## Test AI Assistant integration with TigerMCP

Test what Tools your AI assistance have access to (example for Gemini CLI):

```text
$ gemini
..
> what tools do you have access to?
```

Result should include tools like `semantic-search-tiger-docs` and `service-fork`.

## Generate Data

Run the `generate_sensor_data.py` script. It will create two CSV data files that you'll be working on for the rest of the workshop.

By the way, this script was generated by Claude code, using the following prompt:

```text
generate python script that would create csv files with timeseries data:
*data.csv:*
- Row format: `timestamp, sensor_id, temperature, humidity`
- 20 sensors
- Each sensor emits data every minute
- 2 months of data
- Data should reflect daily and weekly patterns

*sensors.csv:*
- Row format: `sensor_id, model, location`
- Contains metadata for each sensor
- Locations: room 1, room 2, etc.
```

You can try to create new or use your own dataset if you wish.

## Generate Schema

Enter the following prompt into your tool like Claude Code. Here and below code block contains the prompt to enter in your AI Coding assistant tool like Gemini CLI or Claude Code:

```text
1. Look at the two CSV files in this directory
2. Create Tiger service `agentic-postgres-workshop` if not already created
3. Using Tiger best practices, create an optimized schema for the data
4. Save the schema to `schema.sql`
5. Apply schema to the newly created database
6. Load data from CSV files
```

## Database Analysis

AI Coding agents are extremely good at contructing complex SQL queries for data analysis

### Data analisys

Let's analyze the data

```text
- Query and document tables, hypertables and continuous aggregates in the database

- Count and analyze the volume of data in the sensor readings

- Scan the sensor data for outliers. Explain why each identified data point is considered an anomaly based on its deviation from the typical pattern
```

## Performance analysis

```text
- Compare query perofmance with and without continuouse aggregate
```

## Data Retention Experiment

Experiment with data retention policies. This experiment would add retention 
policy to the hypertable, that would remove large part of sensor data records
from the database. Experiment is run on the fork of the database, so the original
database is not affected

```text
- Consult with Tiger MCP for query syntax details as you go
- Create a fork of the database and continue on the fork
- Capture number of records and oldest record in the `sensor_data` table
- Create data retention policy so data older than 1 month is deleted
- Wait for the first successful run of the retention job
- Capture number of records and oldest record in the `sensor_data` again
- Compare with the previous data
```

## Optimization

```text
Check the chunk sizes, segmentby and suggest optimizations for better performance.
```
